{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources:\n",
    "- https://github.com/Samvid95/AlgoTradingRepoList\n",
    "\n",
    "- https://www.google.com/amp/s/blog.mlq.ai/deep-reinforcement-learning-for-trading-with-tensorflow-2-0/amp/\n",
    "\n",
    "\n",
    "Quant Radio Podcast Ideas:\n",
    "- Transformer model\n",
    "  - really good at long sequences of time series data\n",
    "  - Recency important - continuously retrain your model\n",
    "- The bitter lesson \n",
    "  - https://www.quantitativo.com/p/the-bitter-lesson?utm_source=substack&utm_medium=web&utm_content=embedded-post&triedRedirect=true\n",
    "  - Don't just throw everything at the model, LESS IS MORE, pick a method and train using that style (e.g. momentum)\n",
    "- Learning to Rank Stocks\n",
    "  - https://www.quantitativo.com/p/learning-to-rank\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Raw Data\n",
    "   import csv of High, Low, Open, Close, Volume for 5-10 stocks\n",
    "  - Remove stocks with share price <$5\n",
    "  - Remove stocks with volatility indicator >X\n",
    "\n",
    "# Model 1:\n",
    "  - Simple bulk model\n",
    "    - Leverage the Bitter Lesson\n",
    "    - Maximum data throughput\n",
    "    - minimal human intervention \n",
    "\n",
    "  #### Inputs/Pre-processing:\n",
    "    - 20 log daily returns\n",
    "    - 12 log monthly returns (t-13 -> t-2)\n",
    "    - 1 January flag\n",
    "    - Convert returns to cumulative returns and z-scores for cross-sectional normalization\n",
    "  \n",
    "  #### The Model:\n",
    "    - Stacked RBMs to form an autoencoder, pretrained layer-wise\n",
    "    - Compress input to a low-dim feature space\n",
    "    - Pass to feedforward Neural Network for classification\n",
    "    - grid search with hold-out validation\n",
    "    - Final Architecture:\n",
    "      - 33-40-4-50-2\n",
    "      - 33 inputs, compressed to 4, classified into 2 classes\n",
    "    - \n",
    "\n",
    "  #### Target/Classification\n",
    "    - Above monthly returns\n",
    "    - Below monthly Returns\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: Momentum Indicator Enriched Model \n",
    "  - apply Trading indicators\n",
    "    - Volatility indicator, SMA, EMA, MACD, etc\n",
    "    - Return single Enriched dataframe for regression\n",
    "- Same model as above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3: Ranking Portfolio with LambdaMART\n",
    "https://www.quantitativo.com/p/learning-to-rank\n",
    "\n",
    "https://xgboost.readthedocs.io/en/stable/tutorials/learning_to_rank.html\n",
    "\n",
    "#### Features\n",
    "  - Compute asset scores using past returns, volatility-normalized indicators and momentum signals\n",
    "  - Modular framework - can encorporate additional features beyond momentum\n",
    "  - 6 features are past returns (raw and normalised for different windows), 15 momentum based features (different windows)\n",
    "\n",
    "#### Score Ranking\n",
    "  - Apply LambdaMART to rank stocks based on future expected performance\n",
    "\n",
    "#### Security Selection\n",
    "  - Long the top decile and short the bottome decile based upon rankings\n",
    "  - Increasing to 40 quantiles rather than 10 improves results but increases volatility and drawdowns\n",
    "  - Sharpe ratio optimised at between 30-40 quantiles\n",
    "\n",
    "#### Volatility scaling\n",
    "  - Normalize position sizes based on *ex-ante monthly volatility*, targeting 15% annualized volatility\n",
    "\n",
    "#### Re-Training and Rebalancing\n",
    "  - Rebalance monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ret-m1', 'ret-m2', 'ret-m3', 'ret-m4', 'ret-m5', 'ret-m6', 'ret-m7',\n",
      "       'ret-m8', 'ret-m9', 'ret-m10', 'ret-m11', 'ret-m12', 'ret-d1', 'ret-d2',\n",
      "       'ret-d3', 'ret-d4', 'ret-d5', 'ret-d6', 'ret-d7', 'ret-d8', 'ret-d9',\n",
      "       'ret-d10', 'ret-d11', 'ret-d12', 'ret-d13', 'ret-d14', 'ret-d15',\n",
      "       'ret-d16', 'ret-d17', 'ret-d18', 'ret-d19', 'ret-d20', 'unadj_close',\n",
      "       'is_next_jan', 'next_month_ret'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "def get_data(symbol):\n",
    "    # Get raw price data (adjusted and unadjusted closes)\n",
    "    raw_df = pd.read_csv(fr\"C:\\Users\\dougl\\Desktop\\Personal Projects\\Deep Learning Algorithm\\RAW DATA\\{symbol}\")\n",
    "\n",
    "    raw_df = raw_df.replace(0, np.nan)\n",
    "    raw_df = raw_df.ffill()         # Foward Fill values to patch any gaps in time series data\n",
    "    raw_df['Date'] = pd.to_datetime(raw_df['Date'], dayfirst=True)\n",
    "    raw_df.set_index('Date', inplace=True)  # Set datetime index here\n",
    "\n",
    "    return raw_df\n",
    "    \n",
    "raw_df = get_data(\"VAS.csv\")  # Example symbol, replace with actual file name\n",
    "\n",
    "def process_data(raw_df, symbol):\n",
    "    \"\"\"\n",
    "    Constructs a feature-rich DataFrame indexed by (date, symbol) for use in predictive models.\n",
    "    Calculates monthly cumulative returns (ret-m1 to ret-m12) based on end-of-month closes and daily cumulative returns (ret-d1 to ret-d20) over the last 20 trading days.\n",
    "    Includes the unadjusted close price, a binary flag is_next_jan indicating if the next month is January, and the forward return for the next month (next_month_ret).\n",
    "    Ensures no missing or infinite values and returns a clean, multi-indexed dataset ready for modeling.\n",
    "    \"\"\"\n",
    "    # Get end-of-month data: last trading day of each month\n",
    "    eom = raw_df.groupby(pd.Grouper(freq='M')).last()\n",
    "    eom['is_next_jan'] = (eom.index.month == 12).astype(int)\n",
    "    eom['next_month_ret'] = eom['Close'].shift(-1) / eom['Close'] - 1\n",
    "\n",
    "    # Compute monthly cumulative returns over the past 12 months (ret-m1 to ret-m12)\n",
    "    monthly = pd.DataFrame(index=eom.index)\n",
    "    for m in range(1, 13):\n",
    "        monthly[f'ret-m{m}'] = (eom['Close'] / eom['Close'].shift(m)) - 1\n",
    "    monthly = monthly.dropna()\n",
    "    monthly = monthly.astype(float)\n",
    "\n",
    "    # Compute daily cumulative returns over the past 20 trading days (ret-d1 to ret-d20)\n",
    "    daily = pd.DataFrame(index=raw_df.index)\n",
    "    for d in range(1, 21):\n",
    "        daily[f'ret-d{d}'] = (raw_df['Close'] / raw_df['Close'].shift(d)) - 1\n",
    "    daily = daily.dropna()\n",
    "    daily = daily.astype(float)\n",
    "\n",
    "    # Align daily to end-of-month dates (last 20 trading days up to each EOM)\n",
    "    # For each EOM date, get the last available daily row <= EOM date\n",
    "    daily_eom = daily.loc[daily.index.isin(eom.index)]\n",
    "    if len(daily_eom) < len(eom):\n",
    "        # If not all EOM dates are present in daily, reindex with forward fill\n",
    "        daily_eom = daily.reindex(eom.index, method='ffill')\n",
    "\n",
    "    # Merge monthly and daily features with EOM targets and metadata\n",
    "    df = monthly.join(daily_eom, how='inner') \\\n",
    "                .join(eom[['Close', 'is_next_jan', 'next_month_ret']], how='inner')\n",
    "    df = df.rename(columns={'Close': 'unadj_close'})\n",
    "\n",
    "    # Clean: drop rows with any missing or infinite values\n",
    "    df = df.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "\n",
    "    # Return None if no usable rows\n",
    "    if len(df) == 0:\n",
    "        return None\n",
    "\n",
    "    # Create MultiIndex (date, symbol)\n",
    "    df.index = pd.MultiIndex.from_tuples([(d, symbol) for d in df.index], names=[\"date\", \"symbol\"])\n",
    "\n",
    "    return df\n",
    "\n",
    "print(process_data(raw_df, \"VAS.csv\").columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now going to loop through each of our stocks using the above function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ALL.csv', 'ALQ.csv', 'ATOM.csv', 'BGL.csv', 'CAR.csv', 'CSL.csv', 'DMP.csv', 'DVP.csv', 'FMG.csv', 'FPH.csv', 'GOLD.csv', 'JHX.csv', 'MIN.csv', 'MQG.csv', 'NWS.csv', 'NXG.csv', 'PDN.csv', 'PME.csv', 'QAN.csv', 'REA.csv', 'RMD.csv', 'SGH.csv', 'STO.csv', 'TWE.csv', 'VAS.csv', 'WCN.csv', 'WDS.csv', 'WHC.csv']\n",
      "(2648, 35)\n",
      "File saved successfully.\n",
      "Index(['ret-m1', 'ret-m2', 'ret-m3', 'ret-m4', 'ret-m5', 'ret-m6', 'ret-m7',\n",
      "       'ret-m8', 'ret-m9', 'ret-m10', 'ret-m11', 'ret-m12', 'ret-d1', 'ret-d2',\n",
      "       'ret-d3', 'ret-d4', 'ret-d5', 'ret-d6', 'ret-d7', 'ret-d8', 'ret-d9',\n",
      "       'ret-d10', 'ret-d11', 'ret-d12', 'ret-d13', 'ret-d14', 'ret-d15',\n",
      "       'ret-d16', 'ret-d17', 'ret-d18', 'ret-d19', 'ret-d20', 'unadj_close',\n",
      "       'is_next_jan', 'next_month_ret'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "folder_path = \"C:\\\\Users\\\\dougl\\\\Desktop\\\\Personal Projects\\\\Deep Learning Algorithm\\\\RAW DATA\\\\\"\n",
    "\n",
    "\n",
    "\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "print(csv_files)\n",
    "\n",
    "full_data = []\n",
    "\n",
    "for stock in csv_files:\n",
    "    # print(f\"Loading {stock}\")\n",
    "    stock_df = get_data(stock)\n",
    "    stock_df = process_data(stock_df, stock)\n",
    "    # If process_data returns None, skip this stock\n",
    "    if stock_df is not None:\n",
    "        full_data.append(stock_df)\n",
    "    # print(f\"{stock} Loaded\")\n",
    "    # print(\"------\")\n",
    "\n",
    "# Combine all stock DataFrames into one\n",
    "full_data = pd.concat(full_data, axis=0)\n",
    "\n",
    "print(full_data.shape)\n",
    "\n",
    "try:\n",
    "    full_data.to_csv('Full_Dataframe.csv', index=True)\n",
    "    print(\"File saved successfully.\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Failed to save 'Full_Dataframe.csv': {e}\")\n",
    "\n",
    "print(full_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing\n",
    "\n",
    "Before training the model, we'll apply several pre-processing steps to clean, standardize features, and define target variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out low-priced stocks - this is a proxy to remove illiquid and noisy stocks. Could enhance this in the future to use the VIX and a $'s traded metric instead.\n",
    "\n",
    "Next apply **Cross-sectional z-score standardization** to all features (except last two columns) to standardize them (I prefer normalisation)\n",
    "\n",
    "Next target variable is assigned a 1 if the next months return is above the median return for that date, otherwise a 0. \n",
    "\n",
    "Preserve the last feature (`is_next_jan`) and the original forward return for later analysis. All components are then combined into the single DataFrame for model input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ret-m1', 'ret-m2', 'ret-m3', 'ret-m4', 'ret-m5', 'ret-m6', 'ret-m7',\n",
      "       'ret-m8', 'ret-m9', 'ret-m10', 'ret-m11', 'ret-m12', 'ret-d1', 'ret-d2',\n",
      "       'ret-d3', 'ret-d4', 'ret-d5', 'ret-d6', 'ret-d7', 'ret-d8', 'ret-d9',\n",
      "       'ret-d10', 'ret-d11', 'ret-d12', 'ret-d13', 'ret-d14', 'ret-d15',\n",
      "       'ret-d16', 'ret-d17', 'ret-d18', 'ret-d19', 'ret-d20', 'is_next_jan',\n",
      "       'target', 'next_month_ret'],\n",
      "      dtype='object')\n",
      "<bound method NDFrame.head of date        symbol \n",
      "2016-06-30  ALL.csv    0.814109\n",
      "2016-07-31  ALL.csv    0.818213\n",
      "2016-08-31  ALL.csv   -0.079368\n",
      "2016-09-30  ALL.csv   -0.021750\n",
      "2016-10-31  ALL.csv    1.040548\n",
      "                         ...   \n",
      "2024-12-31  WHC.csv    1.763361\n",
      "2025-01-31  WHC.csv   -0.173003\n",
      "2025-02-28  WHC.csv    0.474797\n",
      "2025-03-31  WHC.csv    0.069649\n",
      "2025-05-31  WHC.csv   -0.577266\n",
      "Name: ret-d1, Length: 2108, dtype: float64>\n",
      "<bound method NDFrame.tail of date        symbol \n",
      "2016-06-30  ALL.csv    0.814109\n",
      "2016-07-31  ALL.csv    0.818213\n",
      "2016-08-31  ALL.csv   -0.079368\n",
      "2016-09-30  ALL.csv   -0.021750\n",
      "2016-10-31  ALL.csv    1.040548\n",
      "                         ...   \n",
      "2024-12-31  WHC.csv    1.763361\n",
      "2025-01-31  WHC.csv   -0.173003\n",
      "2025-02-28  WHC.csv    0.474797\n",
      "2025-03-31  WHC.csv    0.069649\n",
      "2025-05-31  WHC.csv   -0.577266\n",
      "Name: ret-d1, Length: 2108, dtype: float64>\n"
     ]
    }
   ],
   "source": [
    "# Filter out penny stocks: keep only rows where unadjusted close is \n",
    "# greater than $5\n",
    "raw = full_data[full_data['unadj_close'] > 5]\n",
    "\n",
    "# Drop the unadjusted close column—it’s no longer needed\n",
    "raw = raw.drop(columns=['unadj_close'])\n",
    "\n",
    "# Standardize features (z-score) within each date (cross-sectionally)\n",
    "features_std = raw.iloc[:, :-2].groupby(level=0)\\\n",
    "    .transform(lambda x: (x - x.mean()) / x.std())\n",
    "\n",
    "# Extract the raw (non-standardized) version of the last feature column (is_next_jan)\n",
    "feature_raw = raw.iloc[:, -2]\n",
    "\n",
    "# Binary target: 1 if next_month_ret is above the cross-sectional median \n",
    "# for that date, else 0\n",
    "target = raw.iloc[:, -1]\n",
    "target = (target > target.groupby(level=0).transform('median')).astype(int)\n",
    "target.name = 'target'\n",
    "\n",
    "# Preserve the original next_month_ret values for reference or evaluation\n",
    "next_month_ret = raw.iloc[:, -1]\n",
    "\n",
    "# Concatenate standardized features, raw feature, target, and \n",
    "# forward return into final dataset\n",
    "data = pd.concat([features_std, feature_raw, target, next_month_ret], axis=1)\n",
    "\n",
    "print(data.columns)\n",
    "\n",
    "print(data[\"ret-d1\"].head)\n",
    "print(data[\"ret-d1\"].tail)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation splits\n",
    "To evaluate model performance over time, we implement a **rolling-window cross-validation** framework tailored for time series data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split(data, look_back_years, val_years, validation_first=False):\n",
    "    \"\"\"\n",
    "    Generator that yields rolling train/validation/test splits from a time-indexed DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - data: A pandas DataFrame indexed by date (multi-index allowed, first level must be date)\n",
    "    - look_back_years: Total number of years in the rolling window (train + val)\n",
    "    - val_years: Number of years allocated to validation set\n",
    "    - validation_first: If True, use the order Val → Train → Test; else Train → Val → Test\n",
    "\n",
    "    Yields:\n",
    "    - train_data: training data for current rolling window\n",
    "    - val_data: validation data for current rolling window\n",
    "    - test_data: test data for current rolling window\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a DataFrame with unique dates from the input index. Takes both Stock and Date for index\n",
    "    dt = pd.DataFrame(index=data.index.get_level_values(0).unique())\n",
    "\n",
    "    # Assign each date a corresponding year; shift December forward to avoid lookahead bias\n",
    "    dt['year'] = dt.index\n",
    "    dt['year'] = dt['year'].apply(lambda x: x.year if x.month != 12 else x.year + 1)\n",
    "\n",
    "    # Establish boundaries for rolling window\n",
    "    current_start_year = dt['year'].iloc[0]  # first available year\n",
    "    max_year = dt['year'].max()              # last year available in data\n",
    "\n",
    "    print(f\"Start year: {current_start_year}, Max year: {max_year}\")\n",
    "\n",
    "    while True:\n",
    "        # Determine where to start train and val periods based on validation_first flag\n",
    "        if validation_first:\n",
    "            val_start_year = current_start_year\n",
    "            train_start_year = current_start_year + val_years\n",
    "        else:\n",
    "            train_start_year = current_start_year\n",
    "            val_start_year = current_start_year + (look_back_years - val_years)\n",
    "\n",
    "        test_year = current_start_year + look_back_years\n",
    "        print(test_year, max_year)\n",
    "\n",
    "        # Stop if we've run out of years to create a full test period\n",
    "        if test_year > max_year:\n",
    "            break\n",
    "\n",
    "        # Map year boundaries to actual dates in the original data\n",
    "        val_start = dt[dt['year'] == val_start_year].index.min()\n",
    "        train_start = dt[dt['year'] == train_start_year].index.min()\n",
    "        test_start = dt[dt['year'] == test_year].index.min()\n",
    "        test_end = dt[dt['year'] == test_year].index.max()\n",
    "\n",
    "        # Create masks for filtering the data based on date ranges\n",
    "        if validation_first:\n",
    "            # Validation → Training → Test\n",
    "            val_mask = (data.index.get_level_values(0) > val_start) & \\\n",
    "                       (data.index.get_level_values(0) <= train_start)\n",
    "            train_mask = (data.index.get_level_values(0) > train_start) & \\\n",
    "                         (data.index.get_level_values(0) <= test_start)\n",
    "        else:\n",
    "            # Training → Validation → Test (default)\n",
    "            train_mask = (data.index.get_level_values(0) > train_start) & \\\n",
    "                         (data.index.get_level_values(0) <= val_start)\n",
    "            val_mask = (data.index.get_level_values(0) > val_start) & \\\n",
    "                       (data.index.get_level_values(0) <= test_start)\n",
    "\n",
    "        # Define mask for the test set: follows val/train period\n",
    "        test_mask = (data.index.get_level_values(0) > test_start) & \\\n",
    "                    (data.index.get_level_values(0) <= test_end)\n",
    "\n",
    "        # Apply masks to extract subsets from data\n",
    "        train_data = data.loc[train_mask]\n",
    "        val_data = data.loc[val_mask]\n",
    "        test_data = data.loc[test_mask]\n",
    "\n",
    "        # Yield split data\n",
    "        yield train_data, val_data, test_data\n",
    "\n",
    "        # Advance rolling window by one year for next iteration\n",
    "        current_start_year += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function generates chronological train/validation/test splits by sliding a multi-year window forward one year at a time. for each iteration it defintes the training period, validation period and test year - ensuring no data leakage. This is an implimentation of the Cross-Validation split referenced earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start year: 2016, Max year: 2025\n",
      "2020 2025\n",
      "2021 2025\n",
      "2022 2025\n",
      "2023 2025\n",
      "2024 2025\n",
      "2025 2025\n",
      "2026 2025\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_start</th>\n",
       "      <th>val_end</th>\n",
       "      <th>train_start</th>\n",
       "      <th>train_end</th>\n",
       "      <th>test_start</th>\n",
       "      <th>test_end</th>\n",
       "      <th>training_sampes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-07-31</td>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>2019-01-31</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>2020-01-31</td>\n",
       "      <td>2020-11-30</td>\n",
       "      <td>527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-31</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>2020-01-31</td>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>2021-01-31</td>\n",
       "      <td>2021-11-30</td>\n",
       "      <td>658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-31</td>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>2021-01-31</td>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>2022-11-30</td>\n",
       "      <td>665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-31</td>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>2022-12-31</td>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>2023-11-30</td>\n",
       "      <td>665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-31</td>\n",
       "      <td>2022-12-31</td>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>2023-12-31</td>\n",
       "      <td>2024-01-31</td>\n",
       "      <td>2024-11-30</td>\n",
       "      <td>675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2021-01-31</td>\n",
       "      <td>2023-12-31</td>\n",
       "      <td>2024-01-31</td>\n",
       "      <td>2024-12-31</td>\n",
       "      <td>2025-01-31</td>\n",
       "      <td>2025-05-31</td>\n",
       "      <td>720</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   val_start    val_end train_start  train_end test_start   test_end  \\\n",
       "0 2016-07-31 2018-12-31  2019-01-31 2019-12-31 2020-01-31 2020-11-30   \n",
       "1 2017-01-31 2019-12-31  2020-01-31 2020-12-31 2021-01-31 2021-11-30   \n",
       "2 2018-01-31 2020-12-31  2021-01-31 2021-12-31 2022-01-31 2022-11-30   \n",
       "3 2019-01-31 2021-12-31  2022-01-31 2022-12-31 2023-01-31 2023-11-30   \n",
       "4 2020-01-31 2022-12-31  2023-01-31 2023-12-31 2024-01-31 2024-11-30   \n",
       "5 2021-01-31 2023-12-31  2024-01-31 2024-12-31 2025-01-31 2025-05-31   \n",
       "\n",
       "   training_sampes  \n",
       "0              527  \n",
       "1              658  \n",
       "2              665  \n",
       "3              665  \n",
       "4              675  \n",
       "5              720  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=['val_start', 'val_end', 'train_start', 'train_end', \n",
    "                           'test_start', 'test_end', 'training_sampes'])\n",
    "for train_data, val_data, test_data in train_val_test_split(data, 4, 1):\n",
    "    df.loc[len(df)] = [\n",
    "        train_data.index.get_level_values(0).min(),\n",
    "        train_data.index.get_level_values(0).max(),\n",
    "        val_data.index.get_level_values(0).min(),\n",
    "        val_data.index.get_level_values(0).max(),\n",
    "        test_data.index.get_level_values(0).min(),\n",
    "        test_data.index.get_level_values(0).max(),\n",
    "        len(train_data)\n",
    "    ]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model: FFNN\n",
    "We'll use a simple FeedForwards Neural Network architecture to classify the pre-processed features into two classes, as specified in the paper.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install torch\n",
    "# %pip install torch.nn\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "class FFNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(33, 40)\n",
    "        self.fc2 = nn.Linear(40, 4)\n",
    "        self.fc3 = nn.Linear(4, 50)\n",
    "        self.out = nn.Linear(50, 2)  # logits for 2 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return self.out(x)  # pass logits to CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The FFNN consists of 3 hidden layers with ReLU activations, with a final output layer that produces a binary classification. The input layer is 33 features, which are transformed through progressively deeper representations: froim 40 units, to 4, then to 50, before the output layer of 2. \n",
    "\n",
    "We are using `CrossEntropyLoss`, which is the standard loss function for multi-class classification tasks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.tail of date        symbol \n",
      "2021-01-31  ALL.csv    0.951194\n",
      "2021-02-28  ALL.csv   -1.155492\n",
      "2021-03-31  ALL.csv    0.967002\n",
      "2021-04-30  ALL.csv   -0.100165\n",
      "2021-05-31  ALL.csv   -1.519696\n",
      "                         ...   \n",
      "2023-08-31  WHC.csv   -3.660740\n",
      "2023-09-30  WHC.csv   -2.305724\n",
      "2023-10-31  WHC.csv    1.345617\n",
      "2023-11-30  WHC.csv   -1.111632\n",
      "2023-12-31  WHC.csv    0.035989\n",
      "Name: ret-d1, Length: 720, dtype: float64>\n",
      "torch.Size([512, 33])\n",
      "torch.Size([512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0343, 0.0735],\n",
       "        [0.0294, 0.0557],\n",
       "        [0.0161, 0.0096],\n",
       "        ...,\n",
       "        [0.0309, 0.0745],\n",
       "        [0.0266, 0.0752],\n",
       "        [0.0324, 0.0521]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_dataloader(df, batch_size=512, shuffle=False):\n",
    "    # Convert input features to float32 tensor (all columns except the last two)\n",
    "    X = torch.tensor(df.iloc[:, :-2].astype('float32').values)\n",
    "    # Convert labels (second-to-last column) to int64 tensor\n",
    "    y = torch.tensor(df.iloc[:, -2].astype('int64').values)\n",
    "    # Wrap tensors in a TensorDataset and return a DataLoader\n",
    "    dataset = TensorDataset(X, y)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    \n",
    "print(train_data[\"ret-d1\"].tail)\n",
    "# Create a DataLoader for training with shuffling\n",
    "dataloader = create_dataloader(train_data, shuffle=True)\n",
    "\n",
    "# Fetch one mini-batch from the DataLoader\n",
    "batch_X, batch_y = next(iter(dataloader))\n",
    "\n",
    "# Inspect the shapes of the input features and labels\n",
    "print(batch_X.shape)  # e.g., torch.Size([512, 33])\n",
    "print(batch_y.shape)  # e.g., torch.Size([512])\n",
    "\n",
    "# Initialize the model and run a forward pass on one batch\n",
    "model = FFNN()\n",
    "model(batch_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training: Process and Hyperparameters\n",
    "\n",
    "This section covers the training loop for the Feedforward Neural Network (FFNN) model. The process includes:\n",
    "\n",
    "- **Data Preparation:** Training and validation data are loaded into PyTorch DataLoaders for efficient mini-batch processing.\n",
    "- **Model Initialization:** The FFNN model, optimizer, and loss function are set up.\n",
    "- **Training Loop:** For each epoch, the model is trained on the training set and evaluated on the validation set. The best model (lowest validation loss) is tracked and saved.\n",
    "- **Evaluation:** After training, the model is restored to the best-performing weights.\n",
    "\n",
    "### Key Hyperparameters\n",
    "- **num_epochs:** Number of passes through the training data.  \n",
    "  - *Typical values:* 50, 100, 200\n",
    "- **learning rate (lr):** Step size for optimizer updates.  \n",
    "  - *Typical values:* 1e-3, 1e-4, 5e-4\n",
    "- **batch_size:** Number of samples per mini-batch.  \n",
    "  - *Typical values:* 32, 64, 128, 256, 512\n",
    "- **optimizer:** Algorithm for updating model weights.  \n",
    "  - *Common choices:* Adam, SGD, RMSprop\n",
    "- **loss function:** Measures prediction error.  \n",
    "  - *Common choices for classification:* CrossEntropyLoss, NLLLoss\n",
    "\n",
    "These hyperparameters are often tuned to improve model performance. The defaults here are a good starting point for most classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 01] Train Loss: 0.6928, Train Acc: 0.5194 | Val Loss: 0.6938, Val Acc: 0.5053\n",
      "[Epoch 02] Train Loss: 0.6928, Train Acc: 0.5181 | Val Loss: 0.6938, Val Acc: 0.5053\n",
      "[Epoch 03] Train Loss: 0.6927, Train Acc: 0.5194 | Val Loss: 0.6937, Val Acc: 0.5053\n",
      "[Epoch 04] Train Loss: 0.6927, Train Acc: 0.5181 | Val Loss: 0.6937, Val Acc: 0.5053\n",
      "[Epoch 05] Train Loss: 0.6926, Train Acc: 0.5181 | Val Loss: 0.6937, Val Acc: 0.5018\n",
      "[Epoch 06] Train Loss: 0.6926, Train Acc: 0.5153 | Val Loss: 0.6937, Val Acc: 0.4947\n",
      "[Epoch 07] Train Loss: 0.6926, Train Acc: 0.5167 | Val Loss: 0.6936, Val Acc: 0.4912\n",
      "[Epoch 08] Train Loss: 0.6925, Train Acc: 0.5167 | Val Loss: 0.6936, Val Acc: 0.4912\n",
      "[Epoch 09] Train Loss: 0.6925, Train Acc: 0.5181 | Val Loss: 0.6936, Val Acc: 0.4912\n",
      "[Epoch 10] Train Loss: 0.6924, Train Acc: 0.5181 | Val Loss: 0.6936, Val Acc: 0.4912\n",
      "[Epoch 11] Train Loss: 0.6924, Train Acc: 0.5194 | Val Loss: 0.6936, Val Acc: 0.4912\n",
      "[Epoch 12] Train Loss: 0.6924, Train Acc: 0.5167 | Val Loss: 0.6936, Val Acc: 0.4912\n",
      "[Epoch 13] Train Loss: 0.6924, Train Acc: 0.5181 | Val Loss: 0.6936, Val Acc: 0.4912\n",
      "[Epoch 14] Train Loss: 0.6923, Train Acc: 0.5181 | Val Loss: 0.6936, Val Acc: 0.4947\n",
      "[Epoch 15] Train Loss: 0.6923, Train Acc: 0.5208 | Val Loss: 0.6936, Val Acc: 0.4947\n",
      "[Epoch 16] Train Loss: 0.6923, Train Acc: 0.5208 | Val Loss: 0.6936, Val Acc: 0.4947\n",
      "[Epoch 17] Train Loss: 0.6922, Train Acc: 0.5222 | Val Loss: 0.6936, Val Acc: 0.4947\n",
      "[Epoch 18] Train Loss: 0.6922, Train Acc: 0.5222 | Val Loss: 0.6936, Val Acc: 0.4982\n",
      "[Epoch 19] Train Loss: 0.6922, Train Acc: 0.5222 | Val Loss: 0.6936, Val Acc: 0.4982\n",
      "[Epoch 20] Train Loss: 0.6921, Train Acc: 0.5236 | Val Loss: 0.6935, Val Acc: 0.5018\n",
      "[Epoch 21] Train Loss: 0.6921, Train Acc: 0.5236 | Val Loss: 0.6935, Val Acc: 0.5018\n",
      "[Epoch 22] Train Loss: 0.6921, Train Acc: 0.5264 | Val Loss: 0.6935, Val Acc: 0.4982\n",
      "[Epoch 23] Train Loss: 0.6920, Train Acc: 0.5264 | Val Loss: 0.6935, Val Acc: 0.4982\n",
      "[Epoch 14] Train Loss: 0.6923, Train Acc: 0.5181 | Val Loss: 0.6936, Val Acc: 0.4947\n",
      "[Epoch 15] Train Loss: 0.6923, Train Acc: 0.5208 | Val Loss: 0.6936, Val Acc: 0.4947\n",
      "[Epoch 16] Train Loss: 0.6923, Train Acc: 0.5208 | Val Loss: 0.6936, Val Acc: 0.4947\n",
      "[Epoch 17] Train Loss: 0.6922, Train Acc: 0.5222 | Val Loss: 0.6936, Val Acc: 0.4947\n",
      "[Epoch 18] Train Loss: 0.6922, Train Acc: 0.5222 | Val Loss: 0.6936, Val Acc: 0.4982\n",
      "[Epoch 19] Train Loss: 0.6922, Train Acc: 0.5222 | Val Loss: 0.6936, Val Acc: 0.4982\n",
      "[Epoch 20] Train Loss: 0.6921, Train Acc: 0.5236 | Val Loss: 0.6935, Val Acc: 0.5018\n",
      "[Epoch 21] Train Loss: 0.6921, Train Acc: 0.5236 | Val Loss: 0.6935, Val Acc: 0.5018\n",
      "[Epoch 22] Train Loss: 0.6921, Train Acc: 0.5264 | Val Loss: 0.6935, Val Acc: 0.4982\n",
      "[Epoch 23] Train Loss: 0.6920, Train Acc: 0.5264 | Val Loss: 0.6935, Val Acc: 0.4982\n",
      "[Epoch 24] Train Loss: 0.6920, Train Acc: 0.5250 | Val Loss: 0.6935, Val Acc: 0.4947\n",
      "[Epoch 25] Train Loss: 0.6920, Train Acc: 0.5208 | Val Loss: 0.6935, Val Acc: 0.4947\n",
      "[Epoch 26] Train Loss: 0.6920, Train Acc: 0.5208 | Val Loss: 0.6935, Val Acc: 0.4982\n",
      "[Epoch 27] Train Loss: 0.6919, Train Acc: 0.5222 | Val Loss: 0.6935, Val Acc: 0.5018\n",
      "[Epoch 28] Train Loss: 0.6919, Train Acc: 0.5194 | Val Loss: 0.6934, Val Acc: 0.5018\n",
      "[Epoch 29] Train Loss: 0.6919, Train Acc: 0.5194 | Val Loss: 0.6934, Val Acc: 0.5018\n",
      "[Epoch 30] Train Loss: 0.6918, Train Acc: 0.5181 | Val Loss: 0.6934, Val Acc: 0.5053\n",
      "[Epoch 31] Train Loss: 0.6918, Train Acc: 0.5194 | Val Loss: 0.6934, Val Acc: 0.4982\n",
      "[Epoch 32] Train Loss: 0.6918, Train Acc: 0.5194 | Val Loss: 0.6934, Val Acc: 0.4982\n",
      "[Epoch 33] Train Loss: 0.6917, Train Acc: 0.5208 | Val Loss: 0.6934, Val Acc: 0.4982\n",
      "[Epoch 34] Train Loss: 0.6917, Train Acc: 0.5222 | Val Loss: 0.6934, Val Acc: 0.4947\n",
      "[Epoch 35] Train Loss: 0.6917, Train Acc: 0.5222 | Val Loss: 0.6934, Val Acc: 0.4947\n",
      "[Epoch 36] Train Loss: 0.6917, Train Acc: 0.5222 | Val Loss: 0.6934, Val Acc: 0.4912\n",
      "[Epoch 24] Train Loss: 0.6920, Train Acc: 0.5250 | Val Loss: 0.6935, Val Acc: 0.4947\n",
      "[Epoch 25] Train Loss: 0.6920, Train Acc: 0.5208 | Val Loss: 0.6935, Val Acc: 0.4947\n",
      "[Epoch 26] Train Loss: 0.6920, Train Acc: 0.5208 | Val Loss: 0.6935, Val Acc: 0.4982\n",
      "[Epoch 27] Train Loss: 0.6919, Train Acc: 0.5222 | Val Loss: 0.6935, Val Acc: 0.5018\n",
      "[Epoch 28] Train Loss: 0.6919, Train Acc: 0.5194 | Val Loss: 0.6934, Val Acc: 0.5018\n",
      "[Epoch 29] Train Loss: 0.6919, Train Acc: 0.5194 | Val Loss: 0.6934, Val Acc: 0.5018\n",
      "[Epoch 30] Train Loss: 0.6918, Train Acc: 0.5181 | Val Loss: 0.6934, Val Acc: 0.5053\n",
      "[Epoch 31] Train Loss: 0.6918, Train Acc: 0.5194 | Val Loss: 0.6934, Val Acc: 0.4982\n",
      "[Epoch 32] Train Loss: 0.6918, Train Acc: 0.5194 | Val Loss: 0.6934, Val Acc: 0.4982\n",
      "[Epoch 33] Train Loss: 0.6917, Train Acc: 0.5208 | Val Loss: 0.6934, Val Acc: 0.4982\n",
      "[Epoch 34] Train Loss: 0.6917, Train Acc: 0.5222 | Val Loss: 0.6934, Val Acc: 0.4947\n",
      "[Epoch 35] Train Loss: 0.6917, Train Acc: 0.5222 | Val Loss: 0.6934, Val Acc: 0.4947\n",
      "[Epoch 36] Train Loss: 0.6917, Train Acc: 0.5222 | Val Loss: 0.6934, Val Acc: 0.4912\n",
      "[Epoch 37] Train Loss: 0.6916, Train Acc: 0.5222 | Val Loss: 0.6934, Val Acc: 0.4912\n",
      "[Epoch 38] Train Loss: 0.6916, Train Acc: 0.5264 | Val Loss: 0.6934, Val Acc: 0.4912\n",
      "[Epoch 39] Train Loss: 0.6916, Train Acc: 0.5264 | Val Loss: 0.6933, Val Acc: 0.4912\n",
      "[Epoch 40] Train Loss: 0.6915, Train Acc: 0.5250 | Val Loss: 0.6933, Val Acc: 0.4877\n",
      "[Epoch 41] Train Loss: 0.6915, Train Acc: 0.5250 | Val Loss: 0.6933, Val Acc: 0.4877\n",
      "[Epoch 42] Train Loss: 0.6915, Train Acc: 0.5250 | Val Loss: 0.6933, Val Acc: 0.4877\n",
      "[Epoch 43] Train Loss: 0.6914, Train Acc: 0.5236 | Val Loss: 0.6933, Val Acc: 0.4877\n",
      "[Epoch 44] Train Loss: 0.6914, Train Acc: 0.5250 | Val Loss: 0.6933, Val Acc: 0.4877\n",
      "[Epoch 45] Train Loss: 0.6914, Train Acc: 0.5264 | Val Loss: 0.6933, Val Acc: 0.4877\n",
      "[Epoch 46] Train Loss: 0.6913, Train Acc: 0.5278 | Val Loss: 0.6933, Val Acc: 0.4807\n",
      "[Epoch 47] Train Loss: 0.6913, Train Acc: 0.5264 | Val Loss: 0.6933, Val Acc: 0.4842\n",
      "[Epoch 48] Train Loss: 0.6913, Train Acc: 0.5278 | Val Loss: 0.6933, Val Acc: 0.4877\n",
      "[Epoch 49] Train Loss: 0.6912, Train Acc: 0.5319 | Val Loss: 0.6933, Val Acc: 0.4912\n",
      "[Epoch 50] Train Loss: 0.6912, Train Acc: 0.5333 | Val Loss: 0.6933, Val Acc: 0.4912\n",
      "[Epoch 51] Train Loss: 0.6912, Train Acc: 0.5333 | Val Loss: 0.6933, Val Acc: 0.4912\n",
      "[Epoch 37] Train Loss: 0.6916, Train Acc: 0.5222 | Val Loss: 0.6934, Val Acc: 0.4912\n",
      "[Epoch 38] Train Loss: 0.6916, Train Acc: 0.5264 | Val Loss: 0.6934, Val Acc: 0.4912\n",
      "[Epoch 39] Train Loss: 0.6916, Train Acc: 0.5264 | Val Loss: 0.6933, Val Acc: 0.4912\n",
      "[Epoch 40] Train Loss: 0.6915, Train Acc: 0.5250 | Val Loss: 0.6933, Val Acc: 0.4877\n",
      "[Epoch 41] Train Loss: 0.6915, Train Acc: 0.5250 | Val Loss: 0.6933, Val Acc: 0.4877\n",
      "[Epoch 42] Train Loss: 0.6915, Train Acc: 0.5250 | Val Loss: 0.6933, Val Acc: 0.4877\n",
      "[Epoch 43] Train Loss: 0.6914, Train Acc: 0.5236 | Val Loss: 0.6933, Val Acc: 0.4877\n",
      "[Epoch 44] Train Loss: 0.6914, Train Acc: 0.5250 | Val Loss: 0.6933, Val Acc: 0.4877\n",
      "[Epoch 45] Train Loss: 0.6914, Train Acc: 0.5264 | Val Loss: 0.6933, Val Acc: 0.4877\n",
      "[Epoch 46] Train Loss: 0.6913, Train Acc: 0.5278 | Val Loss: 0.6933, Val Acc: 0.4807\n",
      "[Epoch 47] Train Loss: 0.6913, Train Acc: 0.5264 | Val Loss: 0.6933, Val Acc: 0.4842\n",
      "[Epoch 48] Train Loss: 0.6913, Train Acc: 0.5278 | Val Loss: 0.6933, Val Acc: 0.4877\n",
      "[Epoch 49] Train Loss: 0.6912, Train Acc: 0.5319 | Val Loss: 0.6933, Val Acc: 0.4912\n",
      "[Epoch 50] Train Loss: 0.6912, Train Acc: 0.5333 | Val Loss: 0.6933, Val Acc: 0.4912\n",
      "[Epoch 51] Train Loss: 0.6912, Train Acc: 0.5333 | Val Loss: 0.6933, Val Acc: 0.4912\n",
      "[Epoch 52] Train Loss: 0.6911, Train Acc: 0.5375 | Val Loss: 0.6933, Val Acc: 0.4912\n",
      "[Epoch 53] Train Loss: 0.6911, Train Acc: 0.5375 | Val Loss: 0.6933, Val Acc: 0.4912\n",
      "[Epoch 54] Train Loss: 0.6910, Train Acc: 0.5347 | Val Loss: 0.6933, Val Acc: 0.4912\n",
      "[Epoch 55] Train Loss: 0.6910, Train Acc: 0.5361 | Val Loss: 0.6933, Val Acc: 0.4912\n",
      "[Epoch 56] Train Loss: 0.6910, Train Acc: 0.5403 | Val Loss: 0.6933, Val Acc: 0.4912\n",
      "[Epoch 57] Train Loss: 0.6909, Train Acc: 0.5375 | Val Loss: 0.6933, Val Acc: 0.4877\n",
      "[Epoch 58] Train Loss: 0.6909, Train Acc: 0.5389 | Val Loss: 0.6933, Val Acc: 0.4877\n",
      "[Epoch 59] Train Loss: 0.6909, Train Acc: 0.5361 | Val Loss: 0.6933, Val Acc: 0.4877\n",
      "[Epoch 60] Train Loss: 0.6908, Train Acc: 0.5361 | Val Loss: 0.6933, Val Acc: 0.4877\n",
      "[Epoch 61] Train Loss: 0.6908, Train Acc: 0.5361 | Val Loss: 0.6933, Val Acc: 0.4877\n",
      "[Epoch 62] Train Loss: 0.6907, Train Acc: 0.5361 | Val Loss: 0.6933, Val Acc: 0.4877\n",
      "[Epoch 63] Train Loss: 0.6907, Train Acc: 0.5361 | Val Loss: 0.6933, Val Acc: 0.4877\n",
      "[Epoch 64] Train Loss: 0.6907, Train Acc: 0.5347 | Val Loss: 0.6933, Val Acc: 0.4877\n",
      "[Epoch 65] Train Loss: 0.6906, Train Acc: 0.5347 | Val Loss: 0.6933, Val Acc: 0.4877\n",
      "[Epoch 52] Train Loss: 0.6911, Train Acc: 0.5375 | Val Loss: 0.6933, Val Acc: 0.4912\n",
      "[Epoch 53] Train Loss: 0.6911, Train Acc: 0.5375 | Val Loss: 0.6933, Val Acc: 0.4912\n",
      "[Epoch 54] Train Loss: 0.6910, Train Acc: 0.5347 | Val Loss: 0.6933, Val Acc: 0.4912\n",
      "[Epoch 55] Train Loss: 0.6910, Train Acc: 0.5361 | Val Loss: 0.6933, Val Acc: 0.4912\n",
      "[Epoch 56] Train Loss: 0.6910, Train Acc: 0.5403 | Val Loss: 0.6933, Val Acc: 0.4912\n",
      "[Epoch 57] Train Loss: 0.6909, Train Acc: 0.5375 | Val Loss: 0.6933, Val Acc: 0.4877\n",
      "[Epoch 58] Train Loss: 0.6909, Train Acc: 0.5389 | Val Loss: 0.6933, Val Acc: 0.4877\n",
      "[Epoch 59] Train Loss: 0.6909, Train Acc: 0.5361 | Val Loss: 0.6933, Val Acc: 0.4877\n",
      "[Epoch 60] Train Loss: 0.6908, Train Acc: 0.5361 | Val Loss: 0.6933, Val Acc: 0.4877\n",
      "[Epoch 61] Train Loss: 0.6908, Train Acc: 0.5361 | Val Loss: 0.6933, Val Acc: 0.4877\n",
      "[Epoch 62] Train Loss: 0.6907, Train Acc: 0.5361 | Val Loss: 0.6933, Val Acc: 0.4877\n",
      "[Epoch 63] Train Loss: 0.6907, Train Acc: 0.5361 | Val Loss: 0.6933, Val Acc: 0.4877\n",
      "[Epoch 64] Train Loss: 0.6907, Train Acc: 0.5347 | Val Loss: 0.6933, Val Acc: 0.4877\n",
      "[Epoch 65] Train Loss: 0.6906, Train Acc: 0.5347 | Val Loss: 0.6933, Val Acc: 0.4877\n",
      "[Epoch 66] Train Loss: 0.6906, Train Acc: 0.5333 | Val Loss: 0.6933, Val Acc: 0.4877\n",
      "[Epoch 67] Train Loss: 0.6905, Train Acc: 0.5347 | Val Loss: 0.6933, Val Acc: 0.4877\n",
      "[Epoch 68] Train Loss: 0.6905, Train Acc: 0.5361 | Val Loss: 0.6933, Val Acc: 0.4877\n",
      "[Epoch 69] Train Loss: 0.6904, Train Acc: 0.5431 | Val Loss: 0.6933, Val Acc: 0.4912\n",
      "[Epoch 70] Train Loss: 0.6904, Train Acc: 0.5431 | Val Loss: 0.6933, Val Acc: 0.4947\n",
      "[Epoch 71] Train Loss: 0.6904, Train Acc: 0.5417 | Val Loss: 0.6933, Val Acc: 0.4982\n",
      "[Epoch 72] Train Loss: 0.6903, Train Acc: 0.5444 | Val Loss: 0.6934, Val Acc: 0.4982\n",
      "[Epoch 73] Train Loss: 0.6903, Train Acc: 0.5458 | Val Loss: 0.6934, Val Acc: 0.4982\n",
      "[Epoch 74] Train Loss: 0.6902, Train Acc: 0.5458 | Val Loss: 0.6934, Val Acc: 0.4982\n",
      "[Epoch 75] Train Loss: 0.6902, Train Acc: 0.5472 | Val Loss: 0.6934, Val Acc: 0.4982\n",
      "[Epoch 76] Train Loss: 0.6901, Train Acc: 0.5486 | Val Loss: 0.6934, Val Acc: 0.5018\n",
      "[Epoch 77] Train Loss: 0.6901, Train Acc: 0.5500 | Val Loss: 0.6934, Val Acc: 0.5088\n",
      "[Epoch 78] Train Loss: 0.6901, Train Acc: 0.5500 | Val Loss: 0.6934, Val Acc: 0.5088\n",
      "[Epoch 79] Train Loss: 0.6900, Train Acc: 0.5431 | Val Loss: 0.6934, Val Acc: 0.5053\n",
      "[Epoch 80] Train Loss: 0.6900, Train Acc: 0.5389 | Val Loss: 0.6934, Val Acc: 0.5088\n",
      "[Epoch 66] Train Loss: 0.6906, Train Acc: 0.5333 | Val Loss: 0.6933, Val Acc: 0.4877\n",
      "[Epoch 67] Train Loss: 0.6905, Train Acc: 0.5347 | Val Loss: 0.6933, Val Acc: 0.4877\n",
      "[Epoch 68] Train Loss: 0.6905, Train Acc: 0.5361 | Val Loss: 0.6933, Val Acc: 0.4877\n",
      "[Epoch 69] Train Loss: 0.6904, Train Acc: 0.5431 | Val Loss: 0.6933, Val Acc: 0.4912\n",
      "[Epoch 70] Train Loss: 0.6904, Train Acc: 0.5431 | Val Loss: 0.6933, Val Acc: 0.4947\n",
      "[Epoch 71] Train Loss: 0.6904, Train Acc: 0.5417 | Val Loss: 0.6933, Val Acc: 0.4982\n",
      "[Epoch 72] Train Loss: 0.6903, Train Acc: 0.5444 | Val Loss: 0.6934, Val Acc: 0.4982\n",
      "[Epoch 73] Train Loss: 0.6903, Train Acc: 0.5458 | Val Loss: 0.6934, Val Acc: 0.4982\n",
      "[Epoch 74] Train Loss: 0.6902, Train Acc: 0.5458 | Val Loss: 0.6934, Val Acc: 0.4982\n",
      "[Epoch 75] Train Loss: 0.6902, Train Acc: 0.5472 | Val Loss: 0.6934, Val Acc: 0.4982\n",
      "[Epoch 76] Train Loss: 0.6901, Train Acc: 0.5486 | Val Loss: 0.6934, Val Acc: 0.5018\n",
      "[Epoch 77] Train Loss: 0.6901, Train Acc: 0.5500 | Val Loss: 0.6934, Val Acc: 0.5088\n",
      "[Epoch 78] Train Loss: 0.6901, Train Acc: 0.5500 | Val Loss: 0.6934, Val Acc: 0.5088\n",
      "[Epoch 79] Train Loss: 0.6900, Train Acc: 0.5431 | Val Loss: 0.6934, Val Acc: 0.5053\n",
      "[Epoch 80] Train Loss: 0.6900, Train Acc: 0.5389 | Val Loss: 0.6934, Val Acc: 0.5088\n",
      "[Epoch 81] Train Loss: 0.6899, Train Acc: 0.5333 | Val Loss: 0.6934, Val Acc: 0.5123\n",
      "[Epoch 82] Train Loss: 0.6899, Train Acc: 0.5292 | Val Loss: 0.6934, Val Acc: 0.5193\n",
      "[Epoch 83] Train Loss: 0.6899, Train Acc: 0.5306 | Val Loss: 0.6934, Val Acc: 0.5193\n",
      "[Epoch 84] Train Loss: 0.6898, Train Acc: 0.5278 | Val Loss: 0.6934, Val Acc: 0.5228\n",
      "[Epoch 85] Train Loss: 0.6898, Train Acc: 0.5319 | Val Loss: 0.6934, Val Acc: 0.5263\n",
      "[Epoch 86] Train Loss: 0.6897, Train Acc: 0.5319 | Val Loss: 0.6934, Val Acc: 0.5228\n",
      "[Epoch 87] Train Loss: 0.6897, Train Acc: 0.5333 | Val Loss: 0.6934, Val Acc: 0.5263\n",
      "[Epoch 88] Train Loss: 0.6897, Train Acc: 0.5333 | Val Loss: 0.6934, Val Acc: 0.5263\n",
      "[Epoch 89] Train Loss: 0.6896, Train Acc: 0.5333 | Val Loss: 0.6934, Val Acc: 0.5263\n",
      "[Epoch 90] Train Loss: 0.6896, Train Acc: 0.5347 | Val Loss: 0.6934, Val Acc: 0.5263\n",
      "[Epoch 91] Train Loss: 0.6895, Train Acc: 0.5347 | Val Loss: 0.6934, Val Acc: 0.5228\n",
      "[Epoch 92] Train Loss: 0.6895, Train Acc: 0.5347 | Val Loss: 0.6934, Val Acc: 0.5228\n",
      "[Epoch 93] Train Loss: 0.6894, Train Acc: 0.5333 | Val Loss: 0.6934, Val Acc: 0.5263\n",
      "[Epoch 94] Train Loss: 0.6894, Train Acc: 0.5319 | Val Loss: 0.6934, Val Acc: 0.5298\n",
      "[Epoch 95] Train Loss: 0.6894, Train Acc: 0.5306 | Val Loss: 0.6934, Val Acc: 0.5298\n",
      "[Epoch 81] Train Loss: 0.6899, Train Acc: 0.5333 | Val Loss: 0.6934, Val Acc: 0.5123\n",
      "[Epoch 82] Train Loss: 0.6899, Train Acc: 0.5292 | Val Loss: 0.6934, Val Acc: 0.5193\n",
      "[Epoch 83] Train Loss: 0.6899, Train Acc: 0.5306 | Val Loss: 0.6934, Val Acc: 0.5193\n",
      "[Epoch 84] Train Loss: 0.6898, Train Acc: 0.5278 | Val Loss: 0.6934, Val Acc: 0.5228\n",
      "[Epoch 85] Train Loss: 0.6898, Train Acc: 0.5319 | Val Loss: 0.6934, Val Acc: 0.5263\n",
      "[Epoch 86] Train Loss: 0.6897, Train Acc: 0.5319 | Val Loss: 0.6934, Val Acc: 0.5228\n",
      "[Epoch 87] Train Loss: 0.6897, Train Acc: 0.5333 | Val Loss: 0.6934, Val Acc: 0.5263\n",
      "[Epoch 88] Train Loss: 0.6897, Train Acc: 0.5333 | Val Loss: 0.6934, Val Acc: 0.5263\n",
      "[Epoch 89] Train Loss: 0.6896, Train Acc: 0.5333 | Val Loss: 0.6934, Val Acc: 0.5263\n",
      "[Epoch 90] Train Loss: 0.6896, Train Acc: 0.5347 | Val Loss: 0.6934, Val Acc: 0.5263\n",
      "[Epoch 91] Train Loss: 0.6895, Train Acc: 0.5347 | Val Loss: 0.6934, Val Acc: 0.5228\n",
      "[Epoch 92] Train Loss: 0.6895, Train Acc: 0.5347 | Val Loss: 0.6934, Val Acc: 0.5228\n",
      "[Epoch 93] Train Loss: 0.6894, Train Acc: 0.5333 | Val Loss: 0.6934, Val Acc: 0.5263\n",
      "[Epoch 94] Train Loss: 0.6894, Train Acc: 0.5319 | Val Loss: 0.6934, Val Acc: 0.5298\n",
      "[Epoch 95] Train Loss: 0.6894, Train Acc: 0.5306 | Val Loss: 0.6934, Val Acc: 0.5298\n",
      "[Epoch 96] Train Loss: 0.6893, Train Acc: 0.5319 | Val Loss: 0.6934, Val Acc: 0.5298\n",
      "[Epoch 97] Train Loss: 0.6893, Train Acc: 0.5306 | Val Loss: 0.6934, Val Acc: 0.5298\n",
      "[Epoch 98] Train Loss: 0.6892, Train Acc: 0.5306 | Val Loss: 0.6934, Val Acc: 0.5298\n",
      "[Epoch 99] Train Loss: 0.6892, Train Acc: 0.5319 | Val Loss: 0.6934, Val Acc: 0.5298\n",
      "[Epoch 100] Train Loss: 0.6891, Train Acc: 0.5306 | Val Loss: 0.6934, Val Acc: 0.5333\n",
      "Best validation loss was 0.6933 at epoch 60\n",
      "[Epoch 96] Train Loss: 0.6893, Train Acc: 0.5319 | Val Loss: 0.6934, Val Acc: 0.5298\n",
      "[Epoch 97] Train Loss: 0.6893, Train Acc: 0.5306 | Val Loss: 0.6934, Val Acc: 0.5298\n",
      "[Epoch 98] Train Loss: 0.6892, Train Acc: 0.5306 | Val Loss: 0.6934, Val Acc: 0.5298\n",
      "[Epoch 99] Train Loss: 0.6892, Train Acc: 0.5319 | Val Loss: 0.6934, Val Acc: 0.5298\n",
      "[Epoch 100] Train Loss: 0.6891, Train Acc: 0.5306 | Val Loss: 0.6934, Val Acc: 0.5333\n",
      "Best validation loss was 0.6933 at epoch 60\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "def train(\n",
    "    train_data, \n",
    "    val_data, \n",
    "    num_epochs=100, \n",
    "    lr=1e-4, \n",
    "    batch_size=512, \n",
    "    silent=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a Feedforward Neural Network (FFNN) on the provided training data and evaluate on validation data.\n",
    "    Tracks the best model based on validation loss and restores the best weights at the end.\n",
    "\n",
    "    Args:\n",
    "        train_data (pd.DataFrame): Training data.\n",
    "        val_data (pd.DataFrame): Validation data.\n",
    "        num_epochs (int): Number of training epochs.\n",
    "        lr (float): Learning rate for the optimizer.\n",
    "        batch_size (int): Mini-batch size for DataLoader.\n",
    "        silent (bool): If False, prints progress each epoch.\n",
    "\n",
    "    Returns:\n",
    "        dict: Contains the trained model, best validation loss, epoch, and accuracy.\n",
    "    \"\"\"\n",
    "    # Create DataLoaders for training and validation sets\n",
    "    train_loader = create_dataloader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = create_dataloader(val_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Initialize model, optimizer, and loss function\n",
    "    model = FFNN()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Helper function to evaluate model on a given DataLoader\n",
    "    def evaluate(loader):\n",
    "        model.eval()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for X, y in loader:\n",
    "                logits = model(X)\n",
    "                loss = criterion(logits, y)\n",
    "                total_loss += loss.item() * X.size(0)\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                correct += (preds == y).sum().item()\n",
    "                total += y.size(0)\n",
    "        avg_loss = total_loss / total\n",
    "        accuracy = correct / total\n",
    "        return avg_loss, accuracy\n",
    "\n",
    "    # Track the best model based on validation loss\n",
    "    best_val_loss = float('inf')\n",
    "    best_accuracy = 0\n",
    "    best_model_weights = None\n",
    "    best_epoch = -1\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(1, 1 + num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(batch_x)\n",
    "            loss = criterion(logits, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * batch_x.size(0)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            correct += (preds == batch_y).sum().item()\n",
    "            total += batch_x.size(0)\n",
    "\n",
    "        avg_train_loss = total_loss / total\n",
    "        train_acc = correct / total\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        val_loss, val_acc = evaluate(val_loader)\n",
    "\n",
    "        # Print training progress if not in silent mode\n",
    "        if not silent:\n",
    "            print(f\"[Epoch {epoch:02d}] \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        # Update best model if validation loss improves\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_accuracy = val_acc\n",
    "            best_model_weights = copy.deepcopy(model.state_dict())\n",
    "            best_epoch = epoch\n",
    "\n",
    "    # Load weights from the best-performing model\n",
    "    model.load_state_dict(best_model_weights)\n",
    "    if not silent:\n",
    "        print(f\"Best validation loss was {best_val_loss:.4f} at epoch {best_epoch}\")\n",
    "\n",
    "    return {\n",
    "        'model': model,\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'best_epoch': best_epoch,\n",
    "        'best_accuracy': best_accuracy\n",
    "    }\n",
    "\n",
    "# Example usage:\n",
    "results = train(train_data, val_data, num_epochs=100, lr=1e-4, batch_size=512, silent=False)\n",
    "trained_model = results['model']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Predictions: Process and Parameters\n",
    "\n",
    "This section describes how to use the trained model to generate predictions on the test set. The process includes:\n",
    "\n",
    "- **Data Preparation:** The test data is loaded into a DataLoader for efficient batch processing.\n",
    "- **Model Evaluation:** The model is set to evaluation mode and predictions are generated for each batch without computing gradients.\n",
    "- **Probability and Class Assignment:** For each sample, the predicted class and the probability of the positive class are computed.\n",
    "- **Quantile Assignment:** Each sample is assigned a quantile label based on its predicted probability, which is useful for ranking and portfolio construction.\n",
    "\n",
    "### Key Parameters\n",
    "- **batch_size:** Number of samples per mini-batch during prediction.  \n",
    "  - *Typical values:* 128, 256, 512 (should match or be a multiple of training batch size)\n",
    "- **num_quantiles:** Number of quantile bins to assign for ranking.  \n",
    "  - *Typical values:* 10 (deciles), 20, 40 (finer granularity for ranking)\n",
    "\n",
    "Assigning quantiles is especially useful in financial applications for constructing long/short portfolios or evaluating model performance across different risk buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      ret-m1    ret-m2    ret-m3    ret-m4    ret-m5  \\\n",
      "date       symbol                                                      \n",
      "2025-01-31 ALL.csv  0.977591  1.383571  1.429739  1.186168  1.263271   \n",
      "2025-02-28 ALL.csv  0.406909  0.901354  1.144721  1.202088  1.125166   \n",
      "2025-03-31 ALL.csv -0.341682  0.038926  0.435025  0.625850  0.919563   \n",
      "2025-04-30 ALL.csv  0.179030 -0.120631  0.179380  0.497325  0.699685   \n",
      "2025-05-31 ALL.csv -1.215512 -0.715020 -0.959728 -0.580422 -0.267160   \n",
      "\n",
      "                      ret-m6    ret-m7    ret-m8    ret-m9   ret-m10  ...  \\\n",
      "date       symbol                                                     ...   \n",
      "2025-01-31 ALL.csv  1.159889  1.356672  1.484772  1.755659  0.315620  ...   \n",
      "2025-02-28 ALL.csv  1.286345  1.166334  1.317940  1.470540  1.738471  ...   \n",
      "2025-03-31 ALL.csv  0.902887  1.130388  1.047060  1.180390  1.412855  ...   \n",
      "2025-04-30 ALL.csv  0.890066  0.829274  1.015785  0.944749  1.091691  ...   \n",
      "2025-05-31 ALL.csv  0.001233  0.272556  0.318445  0.405833  0.434116  ...   \n",
      "\n",
      "                     ret-d17   ret-d18   ret-d19   ret-d20  is_next_jan  \\\n",
      "date       symbol                                                         \n",
      "2025-01-31 ALL.csv  0.639187  1.032967  1.097614  1.151927            0   \n",
      "2025-02-28 ALL.csv  0.579130  0.596025  0.372998  0.406909            0   \n",
      "2025-03-31 ALL.csv -0.483127 -0.540482 -0.527193 -0.404154            0   \n",
      "2025-04-30 ALL.csv -0.120595 -0.048355  0.179030  0.369033            0   \n",
      "2025-05-31 ALL.csv -1.445098 -1.336187 -1.273874 -1.266303            0   \n",
      "\n",
      "                    target  next_month_ret  predicted_class  predicted_prob  \\\n",
      "date       symbol                                                             \n",
      "2025-01-31 ALL.csv       1       -0.045918                0        0.489353   \n",
      "2025-02-28 ALL.csv       0       -0.110264                0        0.493245   \n",
      "2025-03-31 ALL.csv       1        0.041933                0        0.477930   \n",
      "2025-04-30 ALL.csv       0       -0.066427                0        0.486345   \n",
      "2025-05-31 ALL.csv       1        0.034776                0        0.475184   \n",
      "\n",
      "                    quantile  \n",
      "date       symbol             \n",
      "2025-01-31 ALL.csv         6  \n",
      "2025-02-28 ALL.csv         9  \n",
      "2025-03-31 ALL.csv         2  \n",
      "2025-04-30 ALL.csv         5  \n",
      "2025-05-31 ALL.csv         1  \n",
      "\n",
      "[5 rows x 38 columns]\n"
     ]
    }
   ],
   "source": [
    "def generate_predictions(test_data, model, num_quantiles=10):\n",
    "    \"\"\"\n",
    "    Generate class predictions, probabilities, and quantile labels for the test set using a trained model.\n",
    "\n",
    "    Args:\n",
    "        test_data (pd.DataFrame): Test data for prediction.\n",
    "        model (nn.Module): Trained PyTorch model.\n",
    "        num_quantiles (int): Number of quantile bins for ranking predictions.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Test data with added columns for predicted class, probability, and quantile label.\n",
    "    \"\"\"\n",
    "    # Create DataLoader for test data (no shuffling)\n",
    "    test_loader = create_dataloader(test_data, batch_size=512, shuffle=False)\n",
    "\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "\n",
    "    # Perform forward pass on test data without computing gradients\n",
    "    with torch.no_grad():\n",
    "        for batch_x, _ in test_loader:\n",
    "            logits = model(batch_x)  # Output logits: shape [batch_size, 2]\n",
    "            probs = torch.softmax(logits, dim=1)  # Convert to probabilities\n",
    "            preds = torch.argmax(probs, dim=1)    # Predicted class labels\n",
    "\n",
    "            all_preds.append(preds.cpu())\n",
    "            all_probs.append(probs.cpu())\n",
    "\n",
    "    # Concatenate all mini-batch results into full arrays\n",
    "    all_preds = torch.cat(all_preds).numpy()\n",
    "    all_probs = torch.cat(all_probs).numpy()\n",
    "\n",
    "    # Create a copy of the test data and add predictions\n",
    "    td = test_data.copy()\n",
    "    td[\"predicted_class\"] = all_preds\n",
    "    td[\"predicted_prob\"] = all_probs[:, 1]  # Probability of class 1 (positive class)\n",
    "\n",
    "    # Assign quantile labels (1 to num_quantiles) based on predicted probabilities\n",
    "    # NOTE: duplicates='drop' avoids qcut errors when there are tied probabilities\n",
    "    td[\"quantile\"] = td.groupby(level=0)[\"predicted_prob\"] \\\n",
    "        .transform(lambda x: pd.qcut(x, num_quantiles, labels=False, duplicates='drop') + 1)\n",
    "\n",
    "    return td\n",
    "\n",
    "# Example usage:\n",
    "# test_results = generate_predictions(test_data, trained_model, num_quantiles=10)\n",
    "# Display the first few rows of the test results with predictions\n",
    "# print(test_results.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rolling-Window Training, Prediction, and Results Aggregation\n",
    "\n",
    "This section implements a rolling-window cross-validation loop for time series data. For each fold (test year):\n",
    "\n",
    "- **Results Logging:** Training and prediction results, including metadata and performance metrics, are stored for each fold.\n",
    "- **Aggregation:** All test set predictions are combined for final evaluation.\n",
    "\n",
    "### Key Parameters\n",
    "- **look_back_years:** Number of years in the rolling window (train + val).  \n",
    "  - *Typical values:* 4, 5, 10\n",
    "- **val_years:** Number of years for validation in each window.  \n",
    "  - *Typical values:* 1, 2\n",
    "- **num_quantiles:** Number of quantile bins for ranking predictions.  \n",
    "  - *Typical values:* 10, 20, 40\n",
    "\n",
    "The output shows the best val loss, accuracy and epoch for each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start year: 2016, Max year: 2025\n",
      "2026 2025\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     36\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Best val loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[33m'\u001b[39m\u001b[33mbest_val_loss\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, acc \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[33m'\u001b[39m\u001b[33mbest_accuracy\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Combine all test set predictions across folds\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m preds = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# Example: display summary DataFrame and first few rows of predictions\u001b[39;00m\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# print(df)\u001b[39;00m\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# print(preds.head())\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dougl\\miniconda3\\envs\\ENV-StockTrade01\\Lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    326\u001b[39m     warnings.warn(\n\u001b[32m    327\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    328\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    329\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    330\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dougl\\miniconda3\\envs\\ENV-StockTrade01\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:368\u001b[39m, in \u001b[36mconcat\u001b[39m\u001b[34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;129m@deprecate_nonkeyword_arguments\u001b[39m(version=\u001b[38;5;28;01mNone\u001b[39;00m, allowed_args=[\u001b[33m\"\u001b[39m\u001b[33mobjs\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconcat\u001b[39m(\n\u001b[32m    148\u001b[39m     objs: Iterable[NDFrame] | Mapping[HashableT, NDFrame],\n\u001b[32m   (...)\u001b[39m\u001b[32m    157\u001b[39m     copy: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    158\u001b[39m ) -> DataFrame | Series:\n\u001b[32m    159\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    160\u001b[39m \u001b[33;03m    Concatenate pandas objects along a particular axis.\u001b[39;00m\n\u001b[32m    161\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    366\u001b[39m \u001b[33;03m    1   3   4\u001b[39;00m\n\u001b[32m    367\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m368\u001b[39m     op = \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[43m        \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m        \u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlevels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    381\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m op.get_result()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dougl\\miniconda3\\envs\\ENV-StockTrade01\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:425\u001b[39m, in \u001b[36m_Concatenator.__init__\u001b[39m\u001b[34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[39m\n\u001b[32m    422\u001b[39m     objs = \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[32m    424\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs) == \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m425\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo objects to concatenate\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    427\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    428\u001b[39m     objs = \u001b[38;5;28mlist\u001b[39m(com.not_none(*objs))\n",
      "\u001b[31mValueError\u001b[39m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "# Initialize summary DataFrame to store training details for each test year\n",
    "summary_cols = [\n",
    "    'train_start', 'train_end', 'val_start', 'val_end', 'test_start', 'test_end',\n",
    "    'training_samples', 'best_val_loss', 'best_epoch', 'best_accuracy'\n",
    "]\n",
    "df = pd.DataFrame(columns=summary_cols)\n",
    "\n",
    "preds = []\n",
    "\n",
    "# Perform rolling-window training and evaluation\n",
    "for train_data, val_data, test_data in train_val_test_split(data, look_back_years=10, val_years=1):\n",
    "    year = test_data.index.get_level_values(0)[0].year  # Extract test year for logging\n",
    "\n",
    "    # Train model on training and validation sets\n",
    "    result = train(train_data, val_data, silent=True)\n",
    "\n",
    "    # Generate predictions on test set\n",
    "    td = generate_predictions(test_data, result['model'])\n",
    "    preds.append(td)\n",
    "    \n",
    "    # Record metadata and training results for this fold\n",
    "    df.loc[len(df)] = [\n",
    "        train_data.index.get_level_values(0).min(),\n",
    "        train_data.index.get_level_values(0).max(),\n",
    "        val_data.index.get_level_values(0).min(),\n",
    "        val_data.index.get_level_values(0).max(),\n",
    "        test_data.index.get_level_values(0).min(),\n",
    "        test_data.index.get_level_values(0).max(),\n",
    "        len(train_data),\n",
    "        result['best_val_loss'],\n",
    "        result['best_epoch'],\n",
    "        result['best_accuracy'],\n",
    "    ]\n",
    "\n",
    "    # Print training result summary for current year\n",
    "    print(f\"{year} Best val loss {result['best_val_loss']:.4f}, acc {result['best_accuracy']:.4f}\")\n",
    "\n",
    "# Combine all test set predictions across folds\n",
    "preds = pd.concat(preds)\n",
    "\n",
    "# Example: display summary DataFrame and first few rows of predictions\n",
    "# print(df)\n",
    "# print(preds.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ENV-StockTrade01",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
